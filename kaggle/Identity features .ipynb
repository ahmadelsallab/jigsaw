{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom keras.models import Model\nfrom keras.layers import Input, Dense, Embedding, SpatialDropout1D, add, concatenate\nfrom keras.layers import CuDNNLSTM, Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D\nfrom keras.preprocessing import text, sequence\nfrom keras.callbacks import LearningRateScheduler","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trn_limit = 1400000\n#tst_limit = -1\ntrain_df = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv')[:trn_limit]\n#test_df = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')[:tst_limit]\ntest_df = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"EMBEDDING_FILES = [\n    '../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec',\n    '../input/glove840b300dtxt/glove.840B.300d.txt'\n]\nNUM_MODELS = 2\nBATCH_SIZE = 512\nLSTM_UNITS = 128\nDENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\nEPOCHS = 4\nMAX_LEN = 220\nIDENTITY_COLUMNS = [\n    'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n    'muslim', 'black', 'white', 'psychiatric_or_mental_illness'\n]\nAUX_COLUMNS = ['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']\nTEXT_COLUMN = 'comment_text'\nTARGET_COLUMN = 'target'\nCHARS_TO_REMOVE = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n“”’\\'∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—'\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_coefs(word, *arr):\n    return word, np.asarray(arr, dtype='float32')\n\n\ndef load_embeddings(path):\n    with open(path) as f:\n        return dict(get_coefs(*line.strip().split(' ')) for line in f)\n\n\ndef build_matrix(word_index, path):\n    embedding_index = load_embeddings(path)\n    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n    for word, i in word_index.items():\n        try:\n            embedding_matrix[i] = embedding_index[word]\n        except KeyError:\n            pass\n    return embedding_matrix\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import Input, Dropout, Dense, BatchNormalization, Activation, GRU, Embedding, concatenate, Flatten, Lambda\nfrom keras.models import Model\n\n\ndef category_embedding(cat_vars):\n    \"\"\"\n\n    :param cat_vars: cat_vars = [(3,50), (3,50), (3,50), (3,50), (3,50)]--> (num_categories, emb_sz)\n    As a rule of thumb (fast.ai): emb_size = min(50, (num_categories+1)/2 ( or (num_diff_levels+1)/2)\n    Example: days = 0..6, then emb_sz = min(50, 8/2) = 4\n    :type cat_vars:\n    :return:\n    :rtype:\n    \"\"\"\n    # Inputs\n    input_l = Input(shape=[len(cat_vars), 1])\n\n    # Category inputs, by slice with Lambda layer.\n    # We cannot slice the tensor directly as its output will not be Keras layer.\n    category = [Lambda(lambda x: x[:, i])(input_l) for i in range(len(cat_vars))]\n\n    '''\n    for i in range(len(cat_vars)):\n        category.append(Lambda(lambda x: x[:, i])(input_l))\n    '''\n\n    # Apply embedding layers and get emb_outputs\n    emb_category = [Embedding(cat_vars[i][0], cat_vars[i][1])(category[i]) for i in range(len(cat_vars))]\n\n    '''\n    for i in range(len(cat_vars)):\n        emb_category.append(Embedding(cat_vars[i][0], cat_vars[i][1])(category[i]))\n    '''\n\n\n\n    '''\n    concat_l = Flatten()(emb_category[0])\n    for i in range(len(cat_vars) - 1):\n        concat_l = concatenate([concat_l, Flatten()(emb_category[i + 1])])\n    '''\n\n    # We need to flatten since input is len(cat_vars),\n    # 1 => so each emb_category.shape = (emb_sz,1), so we need to flatten the extra 1\n    emb_outs = [Flatten()(emb_category[i]) for i in range(len(cat_vars))]\n\n    # Concatenated layer\n    concat_l = concatenate(emb_outs)\n    # TODO: try average pooling, and and learnable (Dense) merge\n\n    # model\n    model = Model(input_l, concat_l)\n\n    return model\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"\n\nx_train = train_df[TEXT_COLUMN].astype(str)\ny_train = train_df[TARGET_COLUMN].values\ny_aux_train = train_df[AUX_COLUMNS].values\n\nx_test = test_df[TEXT_COLUMN].astype(str)\n\nfor column in IDENTITY_COLUMNS + [TARGET_COLUMN]:\n    train_df[column] = np.where(train_df[column] >= 0.5, True, False)\n\ntokenizer = text.Tokenizer(filters=CHARS_TO_REMOVE)\ntokenizer.fit_on_texts(list(x_train) + list(x_test))\n\nx_train = tokenizer.texts_to_sequences(x_train)\nx_test = tokenizer.texts_to_sequences(x_test)\nx_train = sequence.pad_sequences(x_train, maxlen=MAX_LEN)\nx_test = sequence.pad_sequences(x_test, maxlen=MAX_LEN)\n\n\nsample_weights = np.ones(len(x_train), dtype=np.float32)\nsample_weights += train_df[IDENTITY_COLUMNS].sum(axis=1)\nsample_weights += train_df[TARGET_COLUMN] * (~train_df[IDENTITY_COLUMNS]).sum(axis=1)\nsample_weights += (~train_df[TARGET_COLUMN]) * train_df[IDENTITY_COLUMNS].sum(axis=1) * 5\nsample_weights /= sample_weights.mean()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(tokenizer.word_index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_matrix = np.concatenate([build_matrix(tokenizer.word_index, f) for f in EMBEDDING_FILES], axis=-1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Identity model"},{"metadata":{},"cell_type":"markdown","source":"## Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[IDENTITY_COLUMNS].shape\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[IDENTITY_COLUMNS].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"id_train_df = train_df[IDENTITY_COLUMNS+[TEXT_COLUMN]].dropna()\nid_train_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_id_train = id_train_df[IDENTITY_COLUMNS].values\nx_id_train = id_train_df[TEXT_COLUMN]\n\nx_id_train = tokenizer.texts_to_sequences(x_id_train)\nx_id_train = sequence.pad_sequences(x_id_train, maxlen=MAX_LEN)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(x_id_train.shape)\nprint(y_id_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_id_model(embedding_matrix, num_identity_targets):\n    words = Input(shape=(None,))\n    x = Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=False)(words)\n    x = SpatialDropout1D(0.2)(x)\n    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n\n    hidden = concatenate([\n        GlobalMaxPooling1D()(x),\n        GlobalAveragePooling1D()(x),\n    ])\n    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n    identity_result = Dense(num_identity_targets, activation='sigmoid')(hidden)# Multi-class\n    \n    model = Model(inputs=words, outputs=identity_result)\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"checkpoint_predictions = []\nweights = []\nfrom keras.callbacks import ModelCheckpoint, Callback\nmodel_name = 'id_model'\n#filepath = os.path.join(gdrive_path, 'jigsaw_' + model_name + '.h5')\ncheckpoint = ModelCheckpoint(filepath='./'+model_name+'.h5', monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n\ncallbacks_lst = [checkpoint]\n\nid_model = build_id_model(embedding_matrix, y_id_train.shape[-1])\nprint(id_model.summary())\nid_model.fit(\n    x_id_train,\n    y_id_train,\n    batch_size=BATCH_SIZE,\n    epochs=2,\n    verbose=2,\n    validation_split=0.2,\n    callbacks=callbacks_lst\n    #sample_weight=[sample_weights.values, np.ones_like(sample_weights)],\n    #callbacks=[LearningRateScheduler(lambda _: 1e-3 * (0.55 ** global_epoch))]\n)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Toxity model"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Prepare id features input"},{"metadata":{"trusted":true},"cell_type":"code","source":"#x_id_train = np.array([id_model.predict(x) for x in x_train])\nx_id_train = id_model.predict(x_train)\nx_id_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_id_test = id_model.predict(x_test)\nx_id_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\ndef build_model(embedding_matrix, num_aux_targets, num_identity_targets):\n    \n    # Text input\n    words = Input(shape=(None,))   \n    x = Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=False)(words)\n    x = SpatialDropout1D(0.2)(x)\n    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n\n\n    hidden = concatenate([\n        GlobalMaxPooling1D()(x),\n        GlobalAveragePooling1D()(x),\n        #GlobalMaxPooling1D(x_id),\n        #GlobalAveragePooling1D(x_id),\n    ])\n    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n    x_text = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n    \n    # Identity inputs\n    ids = Input(shape=(num_identity_targets,))\n    x_id = Dense(DENSE_HIDDEN_UNITS)(ids)\n    \n    hidden = concatenate([x_id, x_text])\n    result = Dense(1, activation='sigmoid')(hidden)# Binary\n    aux_result = Dense(num_aux_targets, activation='sigmoid')(hidden)# Multi-class\n    identity_result = Dense(num_identity_targets, activation='sigmoid')(hidden)# Multi-class\n    \n    model = Model(inputs=[words, ids], outputs=[result, aux_result])\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"checkpoint_predictions = []\nweights = []\n\nfrom keras.callbacks import ModelCheckpoint, Callback\nmodel_name = 'toxity_model'\n#filepath = os.path.join(gdrive_path, 'jigsaw_' + model_name + '.h5')\ncheckpoint = ModelCheckpoint(filepath='./'+model_name+'.h5', monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n\ncallbacks_lst = [checkpoint]\n\nfor model_idx in range(NUM_MODELS):\n    model = build_model(embedding_matrix, y_aux_train.shape[-1], y_id_train.shape[-1])\n    for global_epoch in range(EPOCHS):\n        model.fit(\n            [x_train, x_id_train],\n            [y_train, y_aux_train],\n            batch_size=BATCH_SIZE,\n            epochs=1,\n            verbose=2,\n            sample_weight=[sample_weights.values, np.ones_like(sample_weights)],\n            callbacks=[\n                LearningRateScheduler(lambda _: 1e-3 * (0.55 ** global_epoch)),\n                checkpoint,\n            ]\n        )\n        checkpoint_predictions.append(model.predict([x_test, x_id_test], batch_size=2048)[0].flatten())\n        weights.append(2 ** global_epoch)\n\npredictions = np.average(checkpoint_predictions, weights=weights, axis=0)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame.from_dict({\n    'id': test_df.id,\n    'prediction': predictions\n})\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}